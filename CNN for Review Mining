import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model

# Load the Amazon review dataset
data = pd.read_csv("amazon_reviews.csv")

# Split the data into training and testing sets
train_size = int(len(data) * 0.8)
train_reviews = data['review'][:train_size]
train_labels = data['label'][:train_size]
test_reviews = data['review'][train_size:]
test_labels = data['label'][train_size:]

# Tokenize and pad the reviews
max_words = 10000
max_len = 200
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_reviews)
sequences_train = tokenizer.texts_to_sequences(train_reviews)
sequences_test = tokenizer.texts_to_sequences(test_reviews)
x_train = pad_sequences(sequences_train, maxlen=max_len)
x_test = pad_sequences(sequences_test, maxlen=max_len)

# Convert the labels to one-hot encoding
y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)

# Define the CNN model
embedding_dim = 100

input_layer = Input(shape=(max_len,))
x = Embedding(max_words, embedding_dim)(input_layer)
x = Conv1D(128, 5, activation='relu')(x)
# Define the CNN model (continued)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = GlobalMaxPooling1D()(x)
x = Dense(128, activation='relu')(x)
output_layer = Dense(2, activation='softmax')(x)

model = Model(input_layer, output_layer)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))

# Evaluate the model
score, acc = model.evaluate(x_test, y_test, batch_size=128)
print("Test score:", score)
print("Test accuracy:", acc)
